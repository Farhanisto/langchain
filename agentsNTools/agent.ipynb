{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what are agents\n",
    "This are the problem solvers of the AI world, its ai that can make autonomous decisions.\n",
    "in the cases of RAGS and Chains they follow our specific instructions but agents can actually make their own decisions ie they can decide for themselves on what steps to take next.\n",
    "They can pick the right tool to use, an agent can decide when to search the internet when to use a calculator or make an api call.\n",
    "\n",
    "# what are tools\n",
    "This are functions that agents can use in order to complete a specific task.\n",
    "\n",
    "example what is the temparature of paris + 5. the agent will know to first fetch the weather in paris and then add 5 by using a calculator. This is where the concept of ```reACT``` comes in. This pattern is the best pattern for buildin ai agents and is the acronym for Reasoning and Thinking\n",
    "\n",
    "\n",
    "# reACT\n",
    "\n",
    "Reason:- it first thinks\n",
    "Act:- performs actions based on tools\n",
    "observe:- observe what happened\n",
    "\n",
    "Thought:- I need to find the temparature of paris\n",
    "Action: use whether tool\n",
    "observation:- its 20 degree \n",
    "Thought:- Now add 5\n",
    "Action:- uses calculator\n",
    "observation: its 25\n",
    "\n",
    "\n",
    "# steps for creating an agent\n",
    "\n",
    "1. prompt initialization: in this step provide a prompt to the agent eg reAct prompt. This includes\n",
    "\n",
    "    a. An instruction to the LLM about its task eg reasoning step by step and deciding on what tools to use\n",
    "    b. A description of the tools and their usage\n",
    "2. Agent execution: When a user provides a query to the user the LLM interprets the query and generates a response. The LLM doesnt directly call the tools but it suggests what tools to use based on its reasoning\n",
    "\n",
    "3. Tool invocation: The agents framework(langchain) interprets the LLM output to see if it suggest a tool to invoke. If a tool is suggested langchain calls the tool in your code, passing the arguments from the LLM\n",
    "\n",
    "4. Tool Execution: The tool runs in your python code with access to your system resources, it return back to langchain\n",
    "\n",
    "5. LLM response: Langchains sends the tool output back to the LLM for further reasoning or generating an output to the user\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farhan/codebase/langchain/.venv/lib/python3.11/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question, I need to find out the current time in Halifax. \n",
      "\n",
      "Action: get_halifax_time\n",
      "Action Input: None\u001b[0m\u001b[36;1m\u001b[1;3m2025-06-14 11:19:05\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: 11:19:05\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor, tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def main():\n",
    "    # Initialize the OpenAI chat model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\",temperature=0)\n",
    "\n",
    "    # Define the prompt template\n",
    "    # prompt = ChatPromptTemplate.from_messages(\n",
    "    #     [\n",
    "    #         (\"system\", \"You are a helpful assistant.\"),\n",
    "    #         (\"human\", \"{question}\"),\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "\n",
    "    @tool\n",
    "    def get_halifax_time():\n",
    "\n",
    "        \"\"\"Get the current system time in Halifax.\"\"\"\n",
    "        from datetime import datetime\n",
    "        import pytz\n",
    "        halifax_tz = pytz.timezone('America/Halifax')\n",
    "        return datetime.now(halifax_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    query = \"What is the time now in halifax?, just show the time in halifax, no other text\"\n",
    "\n",
    "    # prompt = PromptTemplate.from_template(\n",
    "    #     \"{question}\"\n",
    "    # )\n",
    "\n",
    "    prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "    tools = [get_halifax_time]\n",
    "    agent = create_react_agent(llm, tools=tools, prompt=prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "    # Create the output parser\n",
    "    # output_parser = StrOutputParser()\n",
    "\n",
    "    # Combine the prompt and output parser into a chain\n",
    "    # chain = prompt | llm | output_parser\n",
    "\n",
    "    # Run the chain with a sample question\n",
    "    agent_executor.invoke({\"input\": {query}})\n",
    "    \n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
